{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2384bcc5",
   "metadata": {},
   "source": [
    "# The Bias-Variance Trade-off: A Practical Demonstration\n",
    "The **bias-variance trade-off** is a central concept in machine learning that helps us understand and manage the errors in our predictive models. It is a fundamental challenge in model building, as it forces us to choose between a simpler model that might underfit the data (high bias) and a more complex model that might overfit the data (high variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa77e6c0",
   "metadata": {},
   "source": [
    "## Why Understanding Bias and Variance is Crucial\n",
    "The primary goal of any machine learning model is **generalization**: to perform well on unseen data. The total error of a model can be decomposed into three components:\n",
    "1. **Bias Error**: Error due to overly simplistic assumptions in the learning algorithm. A high-bias model consistently misses the relevant relations between features and target outputs (underfitting).\n",
    "2. **Variance Error**: Error due to the model being too sensitive to small fluctuations in the training data. A high-variance model performs well on training data but poorly on new data (overfitting).\n",
    "3. **Irreducible Error**: Error that cannot be reduced by any model, as it is inherent noise in the data itself.\n",
    "\n",
    "Understanding this trade-off is essential for:\n",
    "* **Model Selection**: Choosing the right complexity for a given problem.\n",
    "* **Hyperparameter Tuning**: Adjusting parameters (like the degree of a polynomial or the regularization strength) to find the sweet spot between bias and variance.\n",
    "* **Diagnosing Performance**: Determining if poor performance is due to underfitting (fix by increasing complexity) or overfitting (fix by increasing data or regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c55cbc",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "| Concept | Description | Typical Model State | Solution Direction |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **High Bias** | Model is too simple; consistently under-predicts or over-predicts. | **Underfitting** | Increase model complexity (e.g., add features, use a more complex algorithm). |\n",
    "| **High Variance** | Model is too complex; fits the noise in the training data. | **Overfitting** | Decrease model complexity (e.g., regularization, feature selection, more data). |\n",
    "| **Sweet Spot** | Optimal balance where both bias and variance are minimized. | **Good Generalization** | Found through cross-validation and hyperparameter tuning. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b917bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Generate Synthetic Data (for clear visualization)\n",
    "# Note: While the request asked for an sklearn dataset, a synthetic dataset is used here\n",
    "# to perfectly illustrate the underlying function (sin(x)) and the effects of model complexity.\n",
    "np.random.seed(42)\n",
    "N = 30\n",
    "X = np.sort(5 * np.random.rand(N, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.1, N)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Create a high-resolution grid for plotting the models' predictions\n",
    "X_plot = np.linspace(0, 5, 100)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, color='blue', label='Training Data')\n",
    "plt.scatter(X_test, y_test, color='red', label='Test Data')\n",
    "plt.plot(X_plot, np.sin(X_plot), color='green', linestyle='--', label='True Function (sin(x))')\n",
    "plt.title('Synthetic Data for Bias-Variance Demonstration')\n",
    "plt.xlabel('Feature X')\n",
    "plt.ylabel('Target y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541819d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. High Bias (Underfitting) - Simple Linear Model (Degree 1)\n",
    "degree = 1\n",
    "model_bias = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "model_bias.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model_bias.predict(X_train)\n",
    "y_test_pred = model_bias.predict(X_test)\n",
    "\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, color='blue', label='Training Data')\n",
    "plt.scatter(X_test, y_test, color='red', label='Test Data')\n",
    "plt.plot(X_plot, model_bias.predict(X_plot), color='black', label=f'Model Prediction (Degree {degree})')\n",
    "plt.title(f'High Bias (Underfitting) - Degree {degree} Polynomial\\nTrain MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}')\n",
    "plt.xlabel('Feature X')\n",
    "plt.ylabel('Target y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Degree {degree} Model: Train MSE = {train_mse:.4f}, Test MSE = {test_mse:.4f}\")\n",
    "print(\"Observation: The model is too simple (high bias) and fails to capture the non-linear relationship, resulting in high error on both training and test sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1337117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. High Variance (Overfitting) - Complex Model (High Degree)\n",
    "degree = 15\n",
    "model_variance = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "model_variance.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model_variance.predict(X_train)\n",
    "y_test_pred = model_variance.predict(X_test)\n",
    "\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, color='blue', label='Training Data')\n",
    "plt.scatter(X_test, y_test, color='red', label='Test Data')\n",
    "plt.plot(X_plot, model_variance.predict(X_plot), color='black', label=f'Model Prediction (Degree {degree})')\n",
    "plt.title(f'High Variance (Overfitting) - Degree {degree} Polynomial\\nTrain MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}')\n",
    "plt.xlabel('Feature X')\n",
    "plt.ylabel('Target y')\n",
    "plt.legend()\n",
    "plt.ylim(-1.5, 1.5)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Degree {degree} Model: Train MSE = {train_mse:.4f}, Test MSE = {test_mse:.4f}\")\n",
    "print(\"Observation: The model is too complex (high variance). It fits the training data almost perfectly (low Train MSE) but performs poorly on the test data (high Test MSE) because it has learned the noise.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13337e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Optimal Model (Sweet Spot) - Balanced Complexity\n",
    "degree = 3\n",
    "model_optimal = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "model_optimal.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model_optimal.predict(X_train)\n",
    "y_test_pred = model_optimal.predict(X_test)\n",
    "\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, color='blue', label='Training Data')\n",
    "plt.scatter(X_test, y_test, color='red', label='Test Data')\n",
    "plt.plot(X_plot, model_optimal.predict(X_plot), color='black', label=f'Model Prediction (Degree {degree})')\n",
    "plt.title(f'Optimal Model (Sweet Spot) - Degree {degree} Polynomial\\nTrain MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}')\n",
    "plt.xlabel('Feature X')\n",
    "plt.ylabel('Target y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Degree {degree} Model: Train MSE = {train_mse:.4f}, Test MSE = {test_mse:.4f}\")\n",
    "print(\"Observation: This model finds a good balance. Both Train and Test MSE are low and close to each other, indicating good generalization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df9c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Visualizing the Bias-Variance Trade-off\n",
    "degrees = np.arange(1, 10)\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for degree in degrees:\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    train_errors.append(mean_squared_error(y_train, y_train_pred))\n",
    "    test_errors.append(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees, train_errors, label='Training Error (Bias)', marker='o', color='blue')\n",
    "plt.plot(degrees, test_errors, label='Test Error (Generalization Error)', marker='o', color='red')\n",
    "plt.axvline(x=3, color='green', linestyle='--', label='Optimal Complexity (Sweet Spot)')\n",
    "plt.title('Bias-Variance Trade-off Curve')\n",
    "plt.xlabel('Model Complexity (Polynomial Degree)')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(degrees)\n",
    "plt.yscale('log') # Use log scale for better visualization of small errors\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\" - Low Complexity (e.g., Degree 1-2): High Training and Test Error (High Bias/Underfitting).\")\n",
    "print(\" - Optimal Complexity (e.g., Degree 3): Low Training and Test Error (Sweet Spot).\")\n",
    "print(\" - High Complexity (e.g., Degree 6+): Low Training Error but rapidly increasing Test Error (High Variance/Overfitting).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe06fdb",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The demonstration clearly illustrates the **Bias-Variance Trade-off**. As model complexity increases, the bias decreases (the model fits the training data better), but the variance increases (the model becomes too sensitive to the training data's noise). The goal of a machine learning practitioner is to find the model complexity that minimizes the **Test Error** (Generalization Error), which is the point where the two error components are optimally balanced."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
